{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eafc4fcf-0a48-41c8-a92e-e77575c87916",
   "metadata": {},
   "source": [
    "# Praktikum Intelligente Sensortechnik 4\n",
    "Tim Tiedemann, Thomas Lehmann, Tobias De Gasparis\n",
    "\n",
    "Version 09.01.2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf12a59-7e4b-4e09-89f9-447fff74170b",
   "metadata": {},
   "source": [
    "# Einfache intelligente Sensoren und Datenvorverarbeitung\n",
    "Im Praktikum 4 geht es um die Verarbeitung hochdimensionaler Daten bzw. das Clustern der Daten.\n",
    "\n",
    "Lesen Sie sich die Aufgaben gut durch. Sollten Sie eine Aufgabe nicht lösen können, so beschreiben Sie zumindest, wie weit Sie gekommen sind und auf welche Weise Sie vorgegangen sind.\n",
    "\n",
    "Beachten Sie auf der methodischen Seite, dass die Schritte der Datenerhebung, der Datenauswertung und der Kommentierung des Ergebnisses ausgeführt werden. Alle Diagramme sind korrekt zu beschriften.\n",
    "\n",
    "Die Aufgaben sind direkt hier als Protokoll zu bearbeiten. Das abgegebene Notebook soll ausführbar sein. Daneben ist der PDF-Export des Notebook mit abzugeben."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0537730a-2b4a-4a61-89f5-8ff590f6dc7a",
   "metadata": {},
   "source": [
    "**Autoren des Protokolls:** Haron Nazari, Anton Tchekov"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4afe8f3-c64f-4c97-b006-4c88cab5638f",
   "metadata": {},
   "source": [
    "# Hintergrund\n",
    "Intelligente Sensoren sollen nicht nur Daten erfassen, sondern direkt eine Klassifikation mit Informationen auf abstrakterer Ebene aus der Umwelt liefern. Beispielsweise soll eine Geste oder Gegenstände erkannt werden und diese Information über die Schnittstelle zu weiteren Systemkomponenten bereitgestellt werden. In diesem Praktikum wird die Leistungsfähigkeit der Klassifikation einfacher Algorithmen des nicht-überwachten Machine Learnings für diese Aufgabe untersucht."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de19e183-f52a-42cb-adce-ee835c36cead",
   "metadata": {},
   "source": [
    "# Vorbereitungsaufgaben\n",
    "## Sensoren mit hochdimensionaler Ausgabe von Daten\n",
    "\n",
    "**Besorgen Sie sich Informationen zu den folgenden Sensoren:**\n",
    "\n",
    "- Hokuyo URG-04LX-UG01\n",
    "- Hokuyo UTM-30LX-EW\n",
    "- Velodyne VLP-16\n",
    "\n",
    "**Um was für einen Sensor handelt es sich jeweils und welches Messprinzip wird verwendet? Was wird gemessen? Was sind hier der Messbereich, die Auflösung und die Sample-/Messrate?**\n",
    "\n",
    "**Über welche Schnittstelle(n) können die Messdaten ausgegeben werden? Wie werden jeweils Betriebsspannung und Datenleitungen angeschlossen?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3568755-8972-40b1-a648-825c9496c3e7",
   "metadata": {},
   "source": [
    "**URG-04LX**\n",
    "\n",
    "Der URG-04LX ist ein Laserscanner, welcher einen Bereich scannt. Der Laser\n",
    "nutzt einen Infrarotsensor und das Sichtfeld beim Scan ist 240 Grad. Er kann\n",
    "Distanzen bis zu 4000 mm messen und die maximale Fehlerrate beträgt 40 mm bei\n",
    "4000 mm Entfernung. Das Messprinzip basiert auf der Phasendifferenz.\n",
    "\n",
    "**Distanz:** Bis zu 4000 mm\n",
    "\n",
    "**Auflösung:** 1 mm\n",
    "\n",
    "**Genauigkeit:**\n",
    "\n",
    "± 1 % des Messwertes bei 1000 - 4000 mm\n",
    "\n",
    "± 10 mm bei 60 - 1000mm\n",
    "\n",
    "**Geschwindigkeit:** 100 ms / Scan\n",
    "\n",
    "**Schnitstellen:** RS-232, USB\n",
    "\n",
    "**Betriebsspannung:** 5 VDC\n",
    "\n",
    "**Pinout von Sensor:**\n",
    "\n",
    "![Pinout von Sensor](urg_04_pinout.png)\n",
    "\n",
    "**UTM-30LX-EW**\n",
    "\n",
    "Der UTM-30LX-EW nutzt eine Laserquelle um für jeden Schritt die Distanz zu\n",
    "messen. Ein Schritt ist 0.25 Grad groß.\n",
    "\n",
    "**Distanz:** 0.1 - 30 m (60 m Maximum nicht garantiert)\n",
    "\n",
    "**Auflösung:** 1 mm\n",
    "\n",
    "**Genauigkeit:**\n",
    "\n",
    "0.1 - 10m: ± 30 mm\n",
    "\n",
    "10 - 30 m: ± 50 mm\n",
    "\n",
    "**Sichtfeld:** 270 Grad\n",
    "\n",
    "**Schrittweite:** 0.25 Grad\n",
    "\n",
    "**Schnitstellen:** Ethernet / Synchronous Output\n",
    "\n",
    "**Geschwindigkeit:** 25 ms\n",
    "\n",
    "**Velodyne VLP-16**\n",
    "\n",
    "Der VLP-16 LiDAR Sensor kann über Ethernet (TCP/IPv4) angesteuert werden.\n",
    "Der Sensor verwendet Time-of-Flight als Messprinzip.\n",
    "\n",
    "Der Sensor ist ein 3D Sensor mit 100 Meter range\n",
    "\n",
    "Die Betriebsspannung ist von 8 bis 19 Volt.\n",
    "Stromverbrauch 0.9 A (8 Watt) in normaler Operation (maximal 3 A).\n",
    "\n",
    "Die Konfiguration des Sensors kann über ein Web-Inferface erfolgen oder\n",
    "mit HTTP-Requests an API-Endpunkten.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a13536-be2f-45f4-a51e-adcec495b336",
   "metadata": {},
   "source": [
    "# Im Labor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6876b9-d62b-4f2e-8963-e48e0cedc2e2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Objektidentifikation in Daten von Laserscannern\n",
    "Mit LiDARs vom Typ Hokuyo URG-04LX-UG01 sollen Sie die Umgebung der Sensoren erfassen und mittels Clustering-Algorithmus auswerten. Sie sollen untersuchen, ob die Objekte der Umgebung durch den Cluster-Algorithmus gefunden werden."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83dfd96-912e-454d-9e64-e51c96e67185",
   "metadata": {},
   "source": [
    "### Datenerfassung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad956e31-648e-4968-ad7f-47e22952feb4",
   "metadata": {
    "tags": []
   },
   "source": [
    "Verwenden Sie in Absprache mit dem Laborbetreuer einen Sensor vom Typ Hokuyo URG-04LX-UG01 für die Datenerfassung. Wie werden Daten erfasst und wie können sie auf diese zugreifen?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf7150b",
   "metadata": {},
   "source": [
    "Die Sensoren sind beide an einen Raspberry PI angeschlossen, mit dem man sich über SSH verbindet.\n",
    "\n",
    "Auf dem Raspberry PI kann man ein Python-Skript ausführen, welches die Daten auf der Konsole\n",
    "ausgibt. Die Sensoren werden mithilfe der hokuyo-Library über eine Serielle Schittstelle\n",
    "mit der Baudrate 19200 angesprochen (an /dev/ttyACM0 und /dev/ttyACM1).\n",
    "\n",
    "Die Daten haben wir dann nach CSV konvertiert."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed41f47-d7ce-481e-8d49-4242769d2456",
   "metadata": {},
   "source": [
    "**Sammeln Sie Daten von einem Scan und versuche Sie die Daten in einer zweidimensionale Karte darzustellen. Welche Form von Koordinatensystem bietet sich hier an und wie können Sie so einen Plot erzeugen? Können Sie Objekte in der Umgebung identifizieren?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb034f08-6329-4a68-9ff4-5205befa7d49",
   "metadata": {},
   "source": [
    "Ein polares Koordinatensystem bietet sich an, da der Sensor Entfernungen in\n",
    "einem bestimmten Winkel liefert, die von einem zentralen Punkt aus gemessen\n",
    "wurden.\n",
    "\n",
    "2D-Karte der Sensordaten von einem Scan:\n",
    "\n",
    "Erster Sensor (ACM0):\n",
    "\n",
    "![image](single_scan1_2d.png)\n",
    "\n",
    "Zweiter Sensor (ACM1):\n",
    "\n",
    "![image](single_scan2_2d.png)\n",
    "\n",
    "Wir haben durch \"Recherche\" herausgefunden in welchem Raum sich die Sensoren befinden.\n",
    "(Zwischen dem Büro von Silke und 709)\n",
    "\n",
    "Der Sensor liefert 0 zurück, wenn die Entfernung außerhalb der Reichweite ist oder die\n",
    "Messung ungültig ist.\n",
    "\n",
    "Man kann vorne eine unebene Oberfläche erkennen, dabei handelt es sich um ein Fenster mit\n",
    "Gardinen davor, mit einem Schlitz in der Mitte, sodass der LiDAR durch das Fenster scannt.\n",
    "\n",
    "Der Raum hat links und rechts eine Tür, welche jeweils von Objekten verdeckt sind und\n",
    "Lücken in den Daten auslösen.\n",
    "\n",
    "Links im Raum sind zwei Schränke nebeneinander, mit einer Lücke dazwischen.\n",
    "\n",
    "Der Raum ist ungefähr 6 Meter lang und 2 Meter breit.\n",
    "\n",
    "Die LiDARs sind etwa 95 cm voneinander entfernt und 180° zueinander gedreht.\n",
    "\n",
    "**Python code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "title = 'Lidar Messdaten 2D Karte'\n",
    "\n",
    "dist = np.array(())\n",
    "angle = np.array(())\n",
    "\n",
    "with open('ACM1_result.txt', newline='') as csvfile:\n",
    "\tdreader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "\tfor row in dreader:\n",
    "\t\tangle = np.append(angle, np.radians(float(row[0])))\n",
    "\t\tdist = np.append(dist, float(row[1]))\n",
    "\n",
    "fig, ax = plt.subplots(subplot_kw={'projection': 'polar'})\n",
    "fig.canvas.manager.set_window_title(title)\n",
    "\n",
    "ax.plot(angle, dist, '.')\n",
    "ax.set_rmax(5600)\n",
    "ax.set_theta_zero_location(\"N\")\n",
    "ax.set_theta_direction(-1)\n",
    "\n",
    "ax.set_rticks([0.5, 1, 1.5, 2])\n",
    "ax.set_rlabel_position(-22.5)\n",
    "ax.grid(True)\n",
    "\n",
    "ax.set_title(title, va='bottom')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b2fe08-bbb6-479d-838f-92537c7ca1de",
   "metadata": {},
   "source": [
    "**Nehmen Sie von zwei LiDARs einen Scan auf. Gehen Sie davon aus, dass einer der Laserscanner im Ursprung eines karthesischen Koordinatensystems (Weltkoordinatensystem) angebracht ist und mit der Mittelachse genau in Richtung der y-Achse ausgerichtet ist. Fügen Sie die Daten des zweiten Laserscanners mittels geeigneter Rotation und Translation (Argumente experimentell bestimmen) in das Weltkoordinatensystem ein, so dass sich ein Gesamtbild in der Karte ergibt. Optional:  Geben Sie die nötige Rotationsmatrix und den Translationsvektor an bzw. bauen Sie Ihre Datentransformation so auf, dass mit einer Rotationsmatrix und einem Translationsvektor gearbeitet wird.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da wir aber zwei Sensoren verwenden, ist es sinnvoll, die Werte aus dem polaren\n",
    "Koordinatensystem in ein kartesisches Koordinatensystem zu  überführen, damit\n",
    "die Berechnungen für die Translation und Rotation des zweiten Sensors\n",
    "einfacher sind.\n",
    "\n",
    "Experimentell bestimmter Translationsvektor:\n",
    "\n",
    "```\n",
    "[   0,   910 ]\n",
    "```\n",
    "\n",
    "Rotationswinkel:\n",
    "\n",
    "```\n",
    "180 °\n",
    "```\n",
    "\n",
    "Rotationsmatrix:\n",
    "\n",
    "```\n",
    "[\n",
    "  [  -1,  0 ]\n",
    "  [   0, -1 ]\n",
    "]\n",
    "```\n",
    "\n",
    "**Gesamtbild 2D-Karte mit Werten beider Sensoren:**\n",
    "\n",
    "![Zusammengesetzte Daten beider Sensoren](both_scan_2d.png)\n",
    "\n",
    "**Python code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "def pol2cart(rho, phi):\n",
    "\tphi += np.pi / 2\n",
    "\tx = rho * np.cos(phi)\n",
    "\ty = rho * np.sin(phi)\n",
    "\tx = -x\n",
    "\treturn(x, y)\n",
    "\n",
    "sensor2_angle = np.radians(-180)\n",
    "sensor2_translation = np.array((0.0, 910))\n",
    "c = np.cos(sensor2_angle)\n",
    "s = np.sin(sensor2_angle)\n",
    "sensor2_rotation = np.array(((c, -s), (s, c)))\n",
    "\n",
    "print(\"Sensor 2 Translation:\")\n",
    "print(sensor2_translation)\n",
    "print(\"Sensor 2 Rotation:\")\n",
    "print(sensor2_rotation)\n",
    "\n",
    "x1 = np.array(())\n",
    "y1 = np.array(())\n",
    "s1_angle = np.array(())\n",
    "s1_dist = np.array(())\n",
    "with open('ACM0_result.txt', newline='') as csvfile:\n",
    "\tdreader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "\tfor row in dreader:\n",
    "\t\ta = np.radians(float(row[0]))\n",
    "\t\td = float(row[1])\n",
    "\t\ts1_angle = np.append(s1_angle, a)\n",
    "\t\ts1_dist = np.append(s1_dist, d)\n",
    "\t\t(x, y) = pol2cart(d, a)\n",
    "\t\tx1 = np.append(x1, x)\n",
    "\t\ty1 = np.append(y1, y)\n",
    "\n",
    "x2 = np.array(())\n",
    "y2 = np.array(())\n",
    "s2_angle = np.array(())\n",
    "s2_dist = np.array(())\n",
    "with open('ACM1_result.txt', newline='') as csvfile:\n",
    "\tdreader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "\tfor row in dreader:\n",
    "\t\ta = np.radians(float(row[0]))\n",
    "\t\td = float(row[1])\n",
    "\t\ts2_angle = np.append(s2_angle, a)\n",
    "\t\ts2_dist = np.append(s2_dist, d)\n",
    "\t\t(x, y) = pol2cart(d, a)\n",
    "\t\tout = sensor2_rotation.dot(np.array((x, y)))\n",
    "\t\tx = out[0]\n",
    "\t\ty = out[1]\n",
    "\t\tx += sensor2_translation[0]\n",
    "\t\ty += sensor2_translation[1]\n",
    "\t\tx2 = np.append(x2, x)\n",
    "\t\ty2 = np.append(y2, y)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.grid(True)\n",
    "\n",
    "ax.set_aspect('equal', adjustable='box')\n",
    "plt.xlim(-4000, 4000)\n",
    "plt.ylim(-4000, 4000)\n",
    "ax.plot(x1, y1, '.')\n",
    "ax.plot(x2, y2, '.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785834b0-dafa-4e01-a1f3-2afa14bffdd3",
   "metadata": {},
   "source": [
    "### Clustering der Daten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b25dd8c-833f-41a9-981b-6e761a4f329f",
   "metadata": {},
   "source": [
    "**Führen Sie nun ein Clustering mittels DBSCAN aus der scikit-learn-Bibliothek auf den gesammelten LiDAR-Daten von einem Sensor durch. Verwenden Sie evtl. erstmal nur einen Teil, z.B. die ersten 100 Sensordimensionen (Entfernungswerte). Finden Sie geeignete Parameter “min_samples” und “eps”. Stellen  Sie die Cluster-Zuordnung der Punkte in der Karte dar.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7815ed17-724e-44ab-889b-e452c45d397c",
   "metadata": {},
   "source": [
    "Wir haben als `eps` 180 gewählt und für `min_samples` 4.\n",
    "\n",
    "Clusterzuordnung von DBSCAN:\n",
    "\n",
    "![DBSCAN Ergebnisse](dbscan.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143bec0a-8543-4a3d-bba4-72ab735d6bbb",
   "metadata": {},
   "source": [
    "**Wieviele Cluster werden in Ihrem Datensatz identifiziert? Welche Objekte könnten es sein?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es werden insgesamt 8 Cluster identifiziert. Es werden die Wände, beide Gardinen und der\n",
    "Schrank als eigener Cluster erkannt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fc883f-e119-4921-88b6-46e2816d46da",
   "metadata": {},
   "source": [
    "**Sollte im LiDAR-Datensatz eine Standardisierung durchgeführt werden? Was spricht dafür und was dagegen? Beachten Sie die Arbeitsweise von DBSCAN.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4559eb5d-9135-4155-b231-78636304b8c7",
   "metadata": {},
   "source": [
    "Es ist eher nicht sinnvoll, eine Standardiserung durchzuführen,\n",
    "da alle Entfernungsmesswerte die gleiche Einheit haben, und die absolute\n",
    "Entfernungsinformation (z.B. in Metern) wichtig ist und bei der\n",
    "Standartisierung verloren gehen würde. Zudem ist DBSCAN resistent gegenüber\n",
    "Ausreißern, da es dichtebasiert arbeitet, weswegen eine Standardisierung\n",
    "nicht notwendig wäre."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90eb9ecc-ff2e-46e8-8945-de4d3f7e8b4c",
   "metadata": {},
   "source": [
    "**Können Sie eine Daumenregel für die Wahl der Parameter von DBSCAN ableiten, so dass Objekte in der Umgebung als getrennte Objekte erkannt werden?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b94919f-aa49-4426-b5a7-b694d326a9d5",
   "metadata": {},
   "source": [
    "DBSCAN benötigt zwei Eingabeparameter, die minimum samples (`min_samples`) und\n",
    "Epsilon (`eps`).\n",
    "\n",
    "Für `min_samples` kann man als Faustregel `2 * dim` wählen, wobei `dim` die Anzahl\n",
    "der Dimensionen des Datensatzes ist. Dabei sollte `min_samples` mindestens `3` sein.\n",
    "Je größer der Datensatz ist, desto größer sollte man `min_samples` wählen.\n",
    "\n",
    "`eps` kann man mithilfe eines k-distance-graph bestimmen, indem man\n",
    "`k = min_samples` setzt und `eps` auf den Wert setzt, bei dem der Graph einen\n",
    "starken Knick zeigt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d84266-6f65-4b82-b379-61048e92f82b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Bewegungserkennung\n",
    "Aus den Daten der Sensoren des Nucleo-Moduls soll nun eine Bewegung identifiziert werden."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e5f84d-849d-4dca-b504-b9cc8ccd354c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Datenerfassung\n",
    "\n",
    "Verwenden Sie im folgenden das IKS01A3-Board und Ihr Programm, wie Sie es zum Sammeln der 12-dimensionalen 1024-Sample Datensätzen verwendet haben.\n",
    "\n",
    "Nehmen Sie einen Datensatz auf, in dem das Modul zunächst still in einer Postion (und Ausrichtung) steht, dann eine Bewegung durchgeführt wird (z.B. Translation und dann Rotation) und das Modul dann still in einer anderen Position (und Ausrichtung) steht (alles innerhalb eines 1024-Sample-Datensatzes). Dokumentieren (Skizze) Sie die Bewegung! (Empfehlung: Filmen Sie die Bewegung, damit Sie ggf. einige Punkte in den Datensätzen bei der Auswertung besser einer Bewegung zuordnen können.)\n",
    "\n",
    "Optional können Sie versuchen, zusätzlich einen Datensatz mit drei Ruhephasen innerhalb der 1024 Samples aufzunehmen.\n",
    "\n",
    "### Auswertung\n",
    "\n",
    "**Stellen Sie sich diesen Datensatz graphisch als Plot (2D/3D) dar und prüfen Sie, ob die verschiedenen Phasen zu erkennen sind. Zeigen Sie hier einen geeigneten Plot/Ausschnitt.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27144c36-96c0-4dd3-b6ab-212297e276c0",
   "metadata": {},
   "source": [
    "**TODO**: Bild erzeugen und plotten\n",
    "\n",
    "Plots des Datensatzes:\n",
    "\n",
    "![Board Bewegung](imuplot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f090954d-eeb0-4f78-9ecd-ac205a8d49a7",
   "metadata": {},
   "source": [
    "**Versuchen Sie mittels Clustering-Algorithmus (k-Means und DBSCAN) die Bewegungen/Bewegungsabschnitte in den Daten zu identifizieren. Welches Verfahren kann wie gut die Bewegung/Bewegungsabschnitte bei welchen Parametern identifizieren? Welche Normierung/Skalierung war sinnvoll? Ist eine Reduktion mit PCA sinnvoll? Stellen Sie die Ergebnisse des Clusterings ggf. auch in Plots über die Zeit dar.**\n",
    "\n",
    "**Fügen Sie hier Ihre Analyse kommentiert ein.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d80827-b979-44df-afd0-a02506641278",
   "metadata": {},
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b480c9-1c93-4354-8989-e7c7892c5ad6",
   "metadata": {},
   "source": [
    "### Sensorsystem\n",
    "\n",
    "**Wenn Sie nun das Sensorboard zusammen mit den Cluster-Algorithmen als Sensorsystem betrachten, welche Information(en) könnte die Schnittstelle des Sensorsystems als Ergebnis des Clustern bereitstellen? \n",
    "Was wäre bei verschiedenen Bewegungen (2 Ruhephasen, 3 Ruhephasen) eine mögliche Ausgabe?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1174c7c9-f92a-44c1-a9b9-57bae3b6621a",
   "metadata": {},
   "source": [
    "Die Schnittstelle könnte einen Zeitabschnitt zurückliefern, der mit `t_start`\n",
    "und `t_end` definiert ist, zusammen mit einer Beschreibung, was in diesem\n",
    "Zeitabschnitt für eine Bewegung stattgefunden hat.\n",
    "\n",
    "z.B.:\n",
    "\n",
    "| t_start | t_end | Art der Bewegung                 |\n",
    "|---------|-------|----------------------------------|\n",
    "|    0.00 |  1.00 | Ruhephase                        |\n",
    "|    1.00 |  1.50 | Bewegung entlang +X und -Z Achse |\n",
    "|    1.50 |  2.00 | Drehung um Y Achse               |\n",
    "|    2.00 |  3.00 | Ruhephase                        |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
